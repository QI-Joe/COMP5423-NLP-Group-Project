{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28772.02s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    }
   ],
   "source": [
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for './bert_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './bert_model' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m local_bert_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./bert_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(local_bert_path)\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(local_bert_path, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2255\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2252\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2253\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2256\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2258\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2259\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2260\u001b[0m     )\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for './bert_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './bert_model' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "# get BERT model\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "model_name = \"bert-base-uncased\"\n",
    "local_bert_path = \"./bert_model\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = BertTokenizer.from_pretrained(local_bert_path)\n",
    "model = BertModel.from_pretrained(local_bert_path, output_hidden_states=True).to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(local_bert_path)\n",
    "# tokenizer.save_pretrained(local_bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "\n",
    "def bert_emb(text: str):\n",
    "    sentence = text\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    sentence_emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    return sentence_emb\n",
    "\n",
    "def cos_sim(a: np.ndarray, b: np.ndarray):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "class MemoryBank(object):\n",
    "    def __init__(self):\n",
    "        super(MemoryBank, self).__init__()\n",
    "        self.query: list[tuple[str, dict[str, Union[np.ndarray|int]]]] = [0]*50\n",
    "        # self.answer: dict[str, np.ndarray] = dict()\n",
    "        self.q_threshold = 0.9\n",
    "        self.max_memory_size = 50\n",
    "        self.max_threshold = 0.98\n",
    "        self.leth_ptr = 0\n",
    "    \n",
    "    def pre_memory_compute(self, question: str):\n",
    "        \"\"\"\n",
    "        Used to pre-compute and find the most simlarity prompt, thus to reduce model generate time\n",
    "        also will proceed on insertion\n",
    "        \"\"\"\n",
    "        self.oversize_deletion()\n",
    "        query_, query_emb = question, bert_emb(question)\n",
    "        query_sim_list = []\n",
    "        for idx in range(self.leth_ptr%self.max_memory_size):\n",
    "            q, ptr = self.query[idx]\n",
    "            query_sim = cos_sim(query_emb, ptr[\"q_emb\"])\n",
    "            if query_sim > self.q_threshold:\n",
    "                self.query[idx][1][\"query_sim_list\"].append((query_, query_sim))\n",
    "                self.query[idx][1][\"query_sim_list\"] = sorted(self.query[idx][1][\"query_sim_list\"], key=lambda x: x[1], reverse=True)\n",
    "            query_sim_list.append((idx, query_sim))\n",
    "        # self.leth_ptr+=1\n",
    "        if len(query_sim_list) < 1:\n",
    "            self.query[self.leth_ptr%self.max_memory_size] = (query_, {\"q_emb\": query_emb, \"ans\": None, \"query_sim_list\": []})\n",
    "            return False\n",
    "        query_sim_list = sorted(query_sim_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        self.query[self.leth_ptr%self.max_memory_size] = (query_, {\"q_emb\": query_emb, \"ans\": None, \"query_sim_list\": query_sim_list})\n",
    "        most_sim = max(query_sim_list, key=lambda x: x[1])\n",
    "        if most_sim[1]>self.max_threshold:\n",
    "            return self.query[most_sim[0]][1][\"ans\"]\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def answer_insert(self, answer:str):\n",
    "        self.query[self.leth_ptr%self.max_memory_size][1][\"ans\"] = answer\n",
    "        self.leth_ptr+=1\n",
    "\n",
    "    def oversize_deletion(self):\n",
    "        if self.leth_ptr > self.max_memory_size:\n",
    "            overfeat_idx = self.leth_ptr%self.max_memory_size\n",
    "            for idx in range(overfeat_idx, self.max_memory_size):\n",
    "                self.query[idx][\"query_sim_list\"] = list(filter(lambda x: x[0]!=overfeat_idx, self.query[idx][\"query_sim_list\"]))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer_sheet(object):\n",
    "    def __init__(self, memory_path: str):\n",
    "        super(Answer_sheet, self).__init__()\n",
    "        self.path = memory_path\n",
    "        self.memory_size = 50\n",
    "        self.cache_: MemoryBank = MemoryBank()\n",
    "        self.load_memory()\n",
    "    \n",
    "    def load_memory(self):\n",
    "        # read the json file\n",
    "        with open(self.path) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for itm in data:\n",
    "            query = itm[\"user_input\"]\n",
    "            answer = itm[\"response\"]\n",
    "            return_data = self.cache_.pre_memory_compute(query)\n",
    "            self.cache_.answer_insert(answer)\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    def get_cache(self):\n",
    "        return self.cache_\n",
    "    \n",
    "    def retrieve_confirm(self, question: str):\n",
    "        quick_answer = self.cache_.pre_memory_compute(question)\n",
    "        # if quick_answer:\n",
    "        #     answer = quick_answer\n",
    "        # else:\n",
    "        #     answer = model_answer(question)\n",
    "        # self.cache_.answer_insert(answer)\n",
    "        # return answer\n",
    "        return quick_answer\n",
    "\n",
    "    def retrieve(self, question: str):\n",
    "        return self.retrieve_confirm(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_file(type: str=\"A\"):\n",
    "    file_name: str = f\"./chat_history_system_{type.upper()}.json\"\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnm = get_local_file(\"A\")\n",
    "answer_method = Answer_sheet(fnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-805KuY00oiV8JRl2ZKgwL7ANQdtdKZMBwPZvl1XqcYvdZqbQ\",\n",
    "    base_url=\"https://api.chatanywhere.tech/v1\"\n",
    ")        \n",
    "# 发送请求\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\":\"user\", \"content\":\"\"}\n",
    "    ]\n",
    ")\n",
    "# 打印响应内容\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joe/anaconda3/lib/python3.11/site-packages/gradio/components/chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Union, List\n",
    "\n",
    "def save_chat_history(system_name, user_input, response):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    chat_record = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"system\": system_name,\n",
    "        \"user_input\": user_input,\n",
    "        \"response\": response\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(f\"chat_history_{system_name}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            chat_history = json.load(f)\n",
    "            if not isinstance(chat_history, list):\n",
    "                chat_history = []\n",
    "    except FileNotFoundError:\n",
    "        chat_history = []\n",
    "\n",
    "    chat_history.append(chat_record)\n",
    "\n",
    "    with open(f\"chat_history_{system_name}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chat_history, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def chatbot_response_1(user_input: str, memory_bank: Answer_sheet):\n",
    "    response_content = memory_bank.cache_.pre_memory_compute(user_input)\n",
    "    \n",
    "    if not response_content:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ]\n",
    "        )\n",
    "        response_content = response.choices[0].message.content\n",
    "    memory_bank.cache_.answer_insert(response_content)\n",
    "    # 保存对话记录\n",
    "    save_chat_history(\"system_A\", user_input, response_content)\n",
    "    return response_content\n",
    "\n",
    "def chatbot_response_2(user_input, memory_bank: Answer_sheet):\n",
    "    response_content = memory_bank.cache_.pre_memory_compute(user_input)\n",
    "\n",
    "    if not response_content:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ]\n",
    "        )\n",
    "        response_content = response.choices[0].message.content\n",
    "    memory_bank.cache_.answer_insert(response_content)\n",
    "    # 保存对话记录\n",
    "    save_chat_history(\"system_B\", user_input, response_content)\n",
    "    return response_content\n",
    "    \n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row() as initial_page:\n",
    "        gr.Markdown(\"## 选择一个问答系统\")\n",
    "        btn1 = gr.Button(\"A\")\n",
    "        btn2 = gr.Button(\"B\")\n",
    "\n",
    "    with gr.Column(visible=False) as qa_page_1:\n",
    "        gr.Markdown(\"## 问答系统:A\")\n",
    "        chatbot1 = gr.Chatbot()\n",
    "        msg1 = gr.Textbox(label=\"输入你的问题\")\n",
    "        submit1 = gr.Button(\"发送\")\n",
    "        back1 = gr.Button(\"返回\")\n",
    "\n",
    "        file_path = get_local_file(\"A\")\n",
    "        retrieve_component = Answer_sheet(file_path)\n",
    "\n",
    "        def respond_1(message, chat_history):\n",
    "            response = chatbot_response_1(message, retrieve_component)\n",
    "            chat_history.append((message, response))\n",
    "            return \"\", chat_history\n",
    "        # 添加回车触发提交\n",
    "        msg1.submit(respond_1, [msg1, chatbot1], [msg1, chatbot1])\n",
    "        submit1.click(respond_1, [msg1, chatbot1], [msg1, chatbot1])\n",
    "\n",
    "    with gr.Column(visible=False) as qa_page_2:\n",
    "        gr.Markdown(\"## 问答系统:B\")\n",
    "        chatbot2 = gr.Chatbot()\n",
    "        msg2 = gr.Textbox(label=\"输入你的问题\")\n",
    "        submit2 = gr.Button(\"发送\")\n",
    "        back2 = gr.Button(\"返回\")\n",
    "\n",
    "        file_path = get_local_file(\"B\")\n",
    "        retrieve_component = Answer_sheet(file_path)\n",
    "\n",
    "        def respond_2(message, chat_history):\n",
    "            response = chatbot_response_2(message, retrieve_component)\n",
    "            chat_history.append((message, response))\n",
    "            return \"\", chat_history\n",
    "\n",
    "        msg2.submit(respond_2, [msg2, chatbot2], [msg2, chatbot2])\n",
    "        submit2.click(respond_2, [msg2, chatbot2], [msg2, chatbot2])\n",
    "\n",
    "    # click button to implement page jumps \n",
    "    def show_qa_page_1():\n",
    "        return gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "    def show_qa_page_2():\n",
    "        return gr.update(visible=False), gr.update(visible=False), gr.update(visible=True)\n",
    "\n",
    "    def go_back():\n",
    "        return gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
    "\n",
    "    btn1.click(show_qa_page_1, [], [initial_page, qa_page_1, qa_page_2])\n",
    "    btn2.click(show_qa_page_2, [], [initial_page, qa_page_1, qa_page_2])\n",
    "    back1.click(go_back, [], [initial_page, qa_page_1, qa_page_2])\n",
    "    back2.click(go_back, [], [initial_page, qa_page_1, qa_page_2])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hi there, do you know how can I get a girlfriend?',\n",
       " {'q_emb': array([ 5.65255396e-02, -2.19656825e-01,  1.10276386e-01, -3.13620232e-02,\n",
       "          3.65164876e-01, -1.46088347e-01,  9.27966833e-02,  7.21849978e-01,\n",
       "         -1.20279223e-01,  1.29396811e-01,  1.16444364e-01, -7.41654634e-01,\n",
       "         -2.93072551e-01,  6.14998281e-01, -1.63040385e-02,  6.06041551e-02,\n",
       "         -1.74761117e-01,  1.92809924e-01, -1.18943015e-02,  5.56279063e-01,\n",
       "          5.10691591e-02,  3.80209118e-01, -2.70124316e-01,  1.08444847e-01,\n",
       "          4.07317698e-01,  5.03449216e-02,  1.73942536e-01,  1.59422740e-01,\n",
       "          2.99515426e-01, -3.95339310e-01,  2.00941786e-01, -2.71441601e-02,\n",
       "         -2.13072002e-01, -2.65172273e-01,  1.24241613e-01, -2.99234279e-02,\n",
       "          2.23774230e-03, -2.34813422e-01, -2.13881820e-01,  2.02464119e-01,\n",
       "         -7.72613287e-01, -2.30122492e-01, -1.32707763e-03,  2.50830740e-01,\n",
       "         -2.84246266e-01, -7.73672044e-01,  4.06138629e-01, -4.54660118e-01,\n",
       "         -2.34646853e-02, -4.15502489e-01, -1.45244867e-01,  2.48773217e-01,\n",
       "         -6.48777544e-01, -4.64749187e-02,  3.52390110e-02,  2.93608040e-01,\n",
       "         -1.64215952e-01, -9.03093338e-01, -6.12708390e-01,  1.29975349e-01,\n",
       "         -1.36041522e-01, -2.89160367e-02,  2.26696000e-01, -5.02034664e-01,\n",
       "         -1.12511422e-02,  1.52021855e-01,  1.11942582e-01,  5.66202760e-01,\n",
       "         -6.97348773e-01, -3.59369069e-02, -2.45004609e-01, -4.24284011e-01,\n",
       "         -6.30458891e-02, -1.00025289e-01, -7.25424737e-02, -1.36191189e-01,\n",
       "         -8.04421976e-02,  1.33610398e-01, -6.86103702e-02,  1.01582363e-01,\n",
       "         -3.33480418e-01,  7.48939335e-01,  7.95525089e-02,  6.35753930e-01,\n",
       "          3.04127812e-01, -1.49412930e-01, -2.42612407e-01,  3.57237369e-01,\n",
       "         -4.09841955e-01,  5.09340465e-01, -8.75061229e-02, -3.95958632e-04,\n",
       "          1.14428557e-01,  1.55304387e-01,  4.66499299e-01, -3.87969851e-01,\n",
       "          1.35218441e-01,  3.31872702e-01,  3.92883956e-01, -1.84807777e-01,\n",
       "          1.11629367e-01, -7.96964467e-01, -5.68331257e-02, -2.88666129e-01,\n",
       "         -3.07365447e-01, -1.24781549e-01,  1.23132259e-01,  4.24785241e-02,\n",
       "          3.89402032e-01, -7.42714480e-02,  2.76516587e-01, -4.23039228e-01,\n",
       "         -6.46994170e-03, -3.36975940e-02, -1.93727627e-01,  4.36088234e-01,\n",
       "          4.72418249e-01, -1.88172415e-01, -9.37512890e-02,  1.17665581e-01,\n",
       "         -2.41520971e-01, -5.66392779e-01, -5.92728481e-02,  5.85633039e-01,\n",
       "          4.27584291e-01,  4.30643409e-01, -9.63426605e-02, -7.08908141e-02,\n",
       "         -1.38751075e-01, -2.55356133e-01,  2.65490301e-02,  3.96346062e-01,\n",
       "          7.06482649e-01, -4.81783539e-01, -2.21718788e-01, -2.38901764e-01,\n",
       "          1.14810586e-01, -1.43578351e-01, -4.15388763e-01, -1.67008266e-01,\n",
       "          8.42976943e-02,  8.27426240e-02,  1.06359171e-02,  2.06038892e-01,\n",
       "          2.50591859e-02,  3.77210170e-01, -3.48320723e-01, -3.27492654e-01,\n",
       "          1.61827534e-01, -6.75843135e-02,  8.44886601e-02,  2.22404692e-02,\n",
       "         -2.35603690e-01, -1.27992034e-01, -5.29323332e-02, -8.79879817e-02,\n",
       "         -7.44021058e-01,  4.95671779e-02,  3.97829637e-02,  2.71649182e-01,\n",
       "          4.25632000e-01, -5.48491776e-01, -4.59362805e-01,  7.49589443e-01,\n",
       "         -2.26758271e-01,  9.24389288e-02,  3.12566251e-01,  4.70562369e-01,\n",
       "         -3.83634925e-01,  8.48270282e-02, -1.88094214e-01,  1.26467779e-01,\n",
       "          6.61158919e-01,  7.61518702e-02,  6.33974746e-02, -2.66485751e-01,\n",
       "          5.48419178e-01,  3.64650637e-01, -6.56014830e-02,  3.22986871e-01,\n",
       "         -9.30956662e-01,  4.09122497e-01, -6.85385391e-02,  1.39170498e-01,\n",
       "          1.04248337e-01,  2.35218108e-02,  3.90027702e-01, -1.58699498e-01,\n",
       "         -2.26224795e-01,  2.31665105e-01, -4.82176214e-01, -2.82139927e-01,\n",
       "         -3.61324161e-01,  3.23152959e-01,  3.79818559e-01, -2.53016293e-01,\n",
       "         -4.75763917e-01, -2.82002985e-01,  2.04562154e-02,  3.02174062e-01,\n",
       "          3.52683514e-02, -3.28881681e-01,  3.00789267e-01,  2.99900472e-01,\n",
       "         -1.69077456e-01,  5.19752979e-01,  3.11596215e-01, -3.78591754e-02,\n",
       "         -1.43510088e-01,  2.12788820e-01, -4.85381514e-01, -1.94948527e-03,\n",
       "          1.48832396e-01,  5.76366372e-02, -2.11832553e-01, -1.12316154e-01,\n",
       "          3.53481770e-01, -7.68013224e-02,  1.59065276e-01,  9.32691395e-02,\n",
       "          2.33725622e-01, -1.50838658e-01, -3.67701560e-01,  2.35251233e-01,\n",
       "          3.56450640e-02,  4.75511521e-01,  3.83767396e-01, -7.03840107e-02,\n",
       "          4.99346316e-01,  2.56589055e-01,  1.50889084e-01,  5.85167073e-02,\n",
       "          3.88075233e-01, -1.18186716e-02, -1.14285551e-01,  3.10760856e-01,\n",
       "          9.74398777e-02, -4.76474553e-01,  3.02896649e-01,  4.15677838e-02,\n",
       "         -2.85138220e-01, -1.37637956e-02,  3.06980282e-01,  3.34746301e-01,\n",
       "         -2.73995161e-01,  3.27462077e-01, -8.62081870e-02, -2.12204143e-01,\n",
       "          2.81330436e-01,  8.52386132e-02, -5.72083712e-01, -3.84872317e-01,\n",
       "         -1.40146479e-01, -4.24096525e-01, -2.25329980e-01,  6.80105314e-02,\n",
       "         -5.43492019e-01,  3.29986811e-01, -1.33656636e-01, -1.57174498e-01,\n",
       "          3.51874053e-01,  7.14570731e-02,  4.06244367e-01,  4.71676916e-01,\n",
       "         -8.73050690e-01,  1.04345769e-01, -6.60068467e-02,  2.21817717e-01,\n",
       "          1.59323514e-01,  4.85106148e-02,  1.70733050e-01, -5.88248670e-02,\n",
       "          2.72515446e-01,  2.24697009e-01, -1.34162113e-01, -4.36600804e-01,\n",
       "          3.18491906e-01,  1.89604744e-01, -3.01878750e-01, -6.53055549e-01,\n",
       "          4.49562997e-01,  5.25128543e-01, -5.76433122e-01, -1.86328605e-01,\n",
       "         -1.23719424e-01, -6.00187890e-02,  3.34536493e-01,  2.45526969e-01,\n",
       "         -2.14801040e-02, -5.10639906e-01, -3.10630202e-01, -1.50954694e-01,\n",
       "         -3.61616999e-01,  3.89729217e-02,  4.14982468e-01, -2.72365510e-01,\n",
       "          7.70070791e-01, -3.57062928e-03, -6.64579123e-02, -1.14718914e-01,\n",
       "          3.31619143e-01, -2.73070872e-01, -9.12519768e-02,  3.79454195e-01,\n",
       "         -1.37667120e-01,  2.85565913e-01,  1.38410926e-01, -8.99088085e-01,\n",
       "         -3.09430504e+00,  5.28559648e-02,  7.80990347e-02, -4.29539353e-01,\n",
       "          8.39086175e-02,  5.58229014e-02,  2.30543856e-02, -3.42325032e-01,\n",
       "         -6.23301208e-01, -1.46328509e-01, -4.29366678e-01,  2.27817774e-01,\n",
       "         -5.08555770e-02,  1.04281686e-01,  1.38069261e-02, -3.54334712e-01,\n",
       "          4.25769329e-01, -6.10945165e-01, -5.89316130e-01,  5.92600048e-01,\n",
       "         -1.81263432e-01,  2.47238219e-01,  1.85768589e-01, -9.34335291e-02,\n",
       "          2.44421527e-01,  8.90812159e-01,  2.22383231e-01, -1.49086073e-01,\n",
       "          2.36758478e-02,  3.15513730e-01, -2.69841552e-01, -2.39804357e-01,\n",
       "          6.19038194e-02,  2.99668729e-01, -8.41274410e-02,  1.89051285e-01,\n",
       "          6.08595163e-02, -3.03185582e-01, -1.09390706e-01, -7.55026788e-02,\n",
       "          2.41822843e-02, -4.56717581e-01,  3.86485100e-01, -8.80942866e-02,\n",
       "          7.25725591e-01, -4.50361848e-01, -2.33205676e-01, -3.82748842e-01,\n",
       "         -1.74222708e-01,  1.82102039e-01, -1.30769387e-01, -3.02071154e-01,\n",
       "          1.59713805e-01,  2.26949587e-01,  2.29875252e-01, -1.72104523e-01,\n",
       "          4.73038763e-01,  9.11809564e-01, -2.14353502e-02, -4.04296905e-01,\n",
       "          3.39411169e-01, -7.81235173e-02,  1.36560842e-01, -2.77430981e-01,\n",
       "         -1.18361756e-01, -3.80059391e-01, -4.42747384e-01, -4.78851050e-01,\n",
       "          1.78697973e-01,  1.40932962e-01, -7.80914947e-02,  5.69106378e-02,\n",
       "         -4.56084400e-01, -9.51447845e-01, -2.29950100e-01,  1.47335902e-01,\n",
       "         -6.69580996e-02,  8.18572119e-02,  8.08665827e-02,  1.75255034e-02,\n",
       "         -1.16258703e-01, -5.33381641e-01,  2.37793356e-01,  1.75885737e-01,\n",
       "         -7.42316023e-02, -5.70417285e-01,  2.05368266e-01, -3.37230772e-01,\n",
       "          4.95461635e-02, -1.50171310e-01,  4.62733418e-01,  3.27458866e-02,\n",
       "         -4.02933396e-02,  3.41339320e-01,  2.28862643e-01,  1.62420720e-02,\n",
       "          2.55976319e-01, -5.32869220e-01, -2.34249178e-02, -1.76801328e-02,\n",
       "         -6.48191571e-02, -9.48372670e-03,  2.21151665e-01, -9.13938954e-02,\n",
       "          5.20145237e-01, -8.92260298e-02, -3.49143982e-01,  1.35654554e-01,\n",
       "         -2.47000337e-01,  4.78680618e-02,  9.57716182e-02, -6.04748607e-01,\n",
       "         -3.81621383e-02,  1.48880098e-03, -1.83818325e-01, -4.22286451e-01,\n",
       "          2.62705147e-01,  1.05203116e+00, -2.34951526e-01,  3.65323782e-01,\n",
       "         -2.23032925e-02,  8.14552307e-01, -3.63295764e-01, -3.50521743e-01,\n",
       "         -8.84376585e-01, -3.43173891e-02, -1.11874513e-01,  1.81666285e-01,\n",
       "         -2.46111885e-01,  2.00743780e-01,  3.55280191e-01, -9.94693860e-03,\n",
       "         -5.76551497e-01,  6.10394515e-02,  1.24965727e-01,  1.55075148e-01,\n",
       "         -3.79006892e-01, -3.94813895e-01, -5.11584878e-02,  3.13017339e-01,\n",
       "         -9.94983241e-02,  2.80524731e-01, -2.39292279e-01, -1.52605195e-02,\n",
       "         -2.81222790e-01,  8.99406374e-02, -7.07427636e-02, -2.16379091e-01,\n",
       "         -3.22106779e-02,  4.78349596e-01, -3.53215098e-01, -8.11773002e-01,\n",
       "         -4.60312814e-02, -4.34767187e-01,  1.72552466e-01,  4.70328592e-02,\n",
       "          5.83901346e-01, -2.52177984e-01, -5.03144860e-02, -4.15406555e-01,\n",
       "          1.99808732e-01, -3.04470509e-01,  2.94365495e-01,  5.70687711e-01,\n",
       "          2.33017474e-01,  4.48501587e-01, -2.37448126e-01,  6.28866777e-02,\n",
       "         -1.72599971e-01,  2.54012167e-01, -3.32324617e-02, -2.16029689e-01,\n",
       "          9.98578742e-02, -2.45261136e-02, -1.75936967e-01,  7.15088665e-01,\n",
       "          1.80785462e-01,  2.47122303e-01, -3.50741297e-01,  3.28370854e-02,\n",
       "         -2.21684039e-01, -7.25221038e-01, -4.03475076e-01, -3.38801816e-02,\n",
       "          2.04545736e-01,  2.04315096e-01,  1.77314550e-01, -2.29299903e-01,\n",
       "         -6.41477779e-02,  1.95083499e-01, -3.84614617e-01,  5.75314045e-01,\n",
       "         -7.66476616e-02, -1.62513018e-01, -4.18119401e-01, -6.90808654e-01,\n",
       "          6.73294246e-01, -1.77343398e-01,  1.83669969e-01,  4.61829126e-01,\n",
       "          8.23271796e-02,  6.28314838e-02, -2.94856817e-01,  4.25629646e-01,\n",
       "          1.19715072e-01, -1.95782602e-01,  1.09216891e-01,  2.83568025e-01,\n",
       "          3.51052672e-01,  3.06461245e-01,  9.99016240e-02, -1.30486622e-01,\n",
       "         -1.57354176e-01, -4.08070907e-02, -1.26719207e-01, -3.66426557e-01,\n",
       "          1.06699809e-01, -2.61188984e-01,  2.48822905e-02, -3.13655734e-01,\n",
       "         -3.22023034e-01, -9.19964388e-02, -1.74971461e-01,  2.45218575e-01,\n",
       "         -7.21723318e-01, -4.20618504e-01, -4.19941157e-01,  9.57797002e-03,\n",
       "         -6.94944680e-01, -6.89927265e-02,  2.75079638e-01, -4.45365399e-01,\n",
       "          2.58204550e-01,  1.91581622e-02, -5.88394739e-02,  5.23465931e-01,\n",
       "         -4.49374691e-02,  7.90239796e-02, -2.60343045e-01,  1.75575972e-01,\n",
       "          8.75254646e-02, -5.54796495e-03,  2.79018402e-01, -3.97206128e-01,\n",
       "         -7.91153312e-02, -4.47746590e-02,  1.77853197e-01,  5.60258150e-01,\n",
       "         -8.42989460e-02, -2.76020944e-01,  1.38925120e-01,  5.20296919e-04,\n",
       "         -3.31802845e-01,  1.97603349e-02,  2.63441920e-01, -2.63159811e-01,\n",
       "         -5.39890230e-01,  2.70039082e-01, -1.02950700e-01,  3.77487957e-01,\n",
       "         -9.74958390e-03,  6.43730816e-03,  1.06395505e-01,  2.10174635e-01,\n",
       "          3.41295630e-01, -6.36981726e-02, -1.90232322e-01, -6.86130002e-02,\n",
       "          1.76977962e-01, -4.04660165e-01, -5.88593125e-01, -1.37446985e-01,\n",
       "          5.52487336e-02,  1.86018869e-02, -1.26402676e-01,  5.39292907e-03,\n",
       "         -1.67274967e-01, -5.14265060e-01, -4.57084514e-02, -3.78146857e-01,\n",
       "         -1.18332140e-01,  6.85494244e-01, -4.14959379e-02, -1.28481269e-01,\n",
       "          5.97155809e-01,  1.52370119e-02,  7.64965862e-02,  2.96019226e-01,\n",
       "         -7.44903982e-02,  1.51695862e-01, -1.43973855e-02,  5.32270849e-01,\n",
       "         -2.97399610e-01,  4.34625387e-01,  5.40221035e-01,  3.07098478e-01,\n",
       "          6.25685275e-01,  2.15715960e-01,  6.32876977e-02,  9.30276290e-02,\n",
       "         -2.01852068e-01, -5.06059974e-02,  2.26144325e-02,  1.60271049e-01,\n",
       "         -1.26817688e-01,  7.04070777e-02,  1.39768291e-02, -4.85849530e-01,\n",
       "          1.54259071e-01,  2.66755968e-01, -2.52680123e-01, -3.03559035e-01,\n",
       "          3.62683162e-02,  3.24164987e-01, -4.35352266e-01, -2.66483158e-01,\n",
       "          3.13782096e-01, -2.15044841e-01, -5.22541404e-01,  1.77310541e-01,\n",
       "          8.67108628e-02, -4.14111406e-01,  5.86704791e-01,  2.03224450e-01,\n",
       "          2.00736403e-01,  4.18181837e-01, -7.11192608e-01,  2.60588378e-02,\n",
       "         -2.00391173e-01,  6.67877972e-01,  6.09588884e-02,  9.74015892e-02,\n",
       "         -2.84906656e-01,  6.64950013e-01, -1.18875273e-01, -4.17655766e-01,\n",
       "          2.30376855e-01, -1.62337080e-01, -7.16855749e-02,  2.20644042e-01,\n",
       "         -5.77134430e-01,  4.19893593e-01, -2.01054551e-02,  7.12275803e-01,\n",
       "          4.21860721e-03,  1.53144330e-01, -2.84452811e-02, -5.38343787e-01,\n",
       "          8.91476095e-01,  3.74320708e-02,  2.37272620e-01,  4.61814165e-01,\n",
       "         -9.11084339e-02, -1.60968214e-01,  7.95954168e-02,  2.45776534e-01,\n",
       "          1.13660991e-01,  1.90007269e-01,  3.41752060e-02,  2.02958118e-02,\n",
       "         -7.75703266e-02, -4.99465406e-01,  1.81453273e-01, -5.87983072e-01,\n",
       "         -1.13012791e-01,  2.00429019e-02,  2.61307150e-01, -2.81810556e-02,\n",
       "         -3.49438041e-01, -2.19444707e-01, -2.73182392e-01,  3.50288272e-01,\n",
       "          2.07304522e-01,  3.79841954e-01, -2.02246681e-01,  4.38661963e-01,\n",
       "         -2.75398433e-01, -2.39775732e-01, -5.76152205e-01, -2.73225754e-01,\n",
       "         -1.19865656e-01,  1.67024031e-01, -4.74065304e-01, -1.99977890e-01,\n",
       "         -1.27852619e-01, -7.73888946e-01,  7.13871941e-02, -2.58914560e-01,\n",
       "         -2.39521288e-03, -2.40950465e-01,  3.57141644e-01, -3.48176599e-01,\n",
       "         -1.99933529e-01, -1.59800753e-01,  2.05593139e-01, -5.18653750e-01,\n",
       "          1.34308696e-01,  3.17246825e-01,  7.46445954e-01,  1.82721302e-01,\n",
       "          3.94930005e-01, -1.99858546e-01,  1.42095283e-01, -1.94156226e-02,\n",
       "         -6.92252219e-02,  1.79968983e-01, -2.00813469e-02,  5.00200272e-01,\n",
       "          3.76050510e-02, -3.95942450e-01,  2.06203554e-02,  5.02930462e-01,\n",
       "         -6.77065313e-01, -3.87787670e-02, -1.85406327e-01,  6.56478181e-02,\n",
       "          4.06359553e-01,  1.09556429e-01, -2.37612382e-01,  3.41434240e-01,\n",
       "         -4.13762063e-01, -5.17009556e-01, -2.58488543e-02,  6.86111972e-02,\n",
       "          1.85665607e-01,  3.27762038e-01,  2.47293353e-01,  3.72798115e-01,\n",
       "         -1.40277490e-01,  5.07985763e-02,  1.57716423e-01,  3.04876029e-01,\n",
       "          1.70947880e-01,  6.13325369e-03, -1.40561387e-01, -1.33141801e-01,\n",
       "          1.97189212e-01,  1.22753568e-02, -2.47837797e-01, -3.70426625e-01,\n",
       "          2.70501345e-01,  2.99873471e-01, -2.16151223e-01, -2.09661275e-01,\n",
       "         -8.53395760e-01,  6.25643134e-01, -1.39514580e-01, -5.13483882e-01,\n",
       "          1.86503261e-01, -6.15436852e-01,  2.43804790e-02, -3.45178664e-01,\n",
       "          2.39681214e-01, -5.54371357e-01, -3.64111662e-01, -8.87136087e-02,\n",
       "         -2.58193221e-02, -4.81211066e-01,  1.31532237e-01,  3.58100593e-01],\n",
       "        dtype=float32),\n",
       "  'ans': 'There is no guaranteed way to get a girlfriend, as every person is unique and relationships develop in different ways. However, some general tips for finding a girlfriend include:\\n\\n1. Be yourself and be confident in who you are.\\n2. Make an effort to socialize and meet new people, whether through hobbies, activities, or online dating.\\n3. Show genuine interest in getting to know someone and building a connection.\\n4. Be respectful, kind, and thoughtful in your interactions with others.\\n5. Communicate openly and honestly about your intentions and feelings.\\n6. Remember that relationships take time to develop, so be patient and open to the process.\\n\\nUltimately, finding a girlfriend is about building a strong connection with someone who aligns with your values, interests, and goals. Focus on being the best version of yourself and let relationships naturally evolve from there.',\n",
       "  'query_sim_list': [(2, 0.99999994),\n",
       "   (3, 0.7128176),\n",
       "   (0, 0.30119222),\n",
       "   (1, 0.30119222)]})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_component.cache_.query[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
